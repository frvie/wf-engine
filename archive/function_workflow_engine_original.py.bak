"""
Function-Based Workflow Orchestrator

A lightweight orchestrator for function-based workflow nodes using a workflow decorator.
Supports parallel execution, dependency resolution, and automatic conflict isolation.
"""

import json
import time
import os
import concurrent.futures
import importlib
import inspect
from typing import Dict, List, Any
from pathlib import Path

from utilities.logging_config import get_workflow_logger


class FunctionWorkflowEngine:
    """Minimal workflow engine for function-based nodes with unified decorator"""
    
    def __init__(self, workflow_data: Dict = None):
        self.logger = get_workflow_logger()
        self.results = {}
        self.environment_manager = None
        
        if workflow_data:
            self.workflow_config = workflow_data.get('workflow', {})
            self.nodes = workflow_data.get('nodes', [])
        else:
            self.workflow_config = {}
            self.nodes = []
        
        # Initialize environment manager
        self._initialize_environment_manager()
        
        # Discover and load required nodes
        self._discover_and_load_nodes()
    
    def _initialize_environment_manager(self):
        """Initialize environment manager for handling dependencies"""
        try:
            from workflow_environment_manager import WorkflowEnvironmentManager
            from pathlib import Path
            
            # Check if environments are configured in workflow
            env_config = self.workflow_config.get('environments', {})
            env_file = env_config.get('file', 'environments.json')
            
            # environments.json is now OPTIONAL - only load if it exists
            if os.path.exists(env_file):
                self.logger.info(f"Loading environments from: {env_file}")
                self.environment_manager = WorkflowEnvironmentManager(
                    environments_file=Path(env_file)
                )
            else:
                # Create environment manager without external file
                # It will auto-generate environments from node dependencies
                self.logger.debug("No environments.json - using auto-generated environments")
                self.environment_manager = WorkflowEnvironmentManager()
                
            self.logger.info("Environment manager initialized")
                
        except Exception as e:
            self.logger.warning(f"Environment manager not available: {e}")
    
    def _discover_and_load_nodes(self):
        """Discover and dynamically load only the nodes needed for this workflow"""
        if not self.nodes:
            return
        
        # Extract required function names from workflow
        required_functions = set()
        for node in self.nodes:
            function_name = node.get('function', '')
            if function_name:
                required_functions.add(function_name)
        
        self.logger.info(f"Discovering nodes for {len(required_functions)} functions...")
        
        # Discover available nodes in workflow_nodes/
        nodes_dir = Path("workflow_nodes")
        if not nodes_dir.exists():
            self.logger.warning(f"Node directory not found: {nodes_dir}")
            return
        
        discovered_nodes = {}
        loaded_count = 0
        
        # Scan for Python files (excluding __init__.py)
        for py_file in nodes_dir.glob("*.py"):
            if py_file.name == '__init__.py':
                continue
                
            try:
                # Convert file path to module name
                module_name = f"workflow_nodes.{py_file.stem}"
                
                # Import the module
                module = importlib.import_module(module_name)
                
                # Find functions with @workflow_node decorator (checks for node_id attribute)
                for name, obj in inspect.getmembers(module):
                    if (inspect.isfunction(obj) and 
                        hasattr(obj, 'node_id')):
                        
                        # Check if this function is needed for the workflow
                        full_function_name = f"{module_name}.{name}"
                        if full_function_name in required_functions:
                            discovered_nodes[full_function_name] = {
                                'function': obj,
                                'module': module_name,
                                'file': py_file.name,
                                'node_id': obj.node_id,
                                'dependencies': getattr(obj, 'dependencies', []),
                                'environment': getattr(obj, 'environment', None)
                            }
                            loaded_count += 1
                            self.logger.debug(f"Loaded: {name} from {py_file.name}")
                        
            except Exception as e:
                self.logger.warning(f"Failed to load {py_file.name}: {e}")
        
        self.logger.info(f"Loaded {loaded_count}/{len(required_functions)} required nodes")
        
        # Store discovered nodes for later use
        self.discovered_nodes = discovered_nodes
    
    def _get_ready_nodes(self, dependency_graph: Dict, completed: set) -> List[str]:
        """Find nodes ready to execute (all dependencies completed)"""
        ready = []
        for node_id, dependencies in dependency_graph.items():
            if node_id not in completed:
                if all(dep in completed for dep in dependencies):
                    ready.append(node_id)
        return ready
    
    def _prepare_inputs(self, node: Dict) -> Dict:
        """Prepare node inputs by merging static inputs with dependency results"""
        inputs = node.get('inputs', {}).copy()
        
        # ONLY auto-inject model_info (common pattern for model loaders â†’ inference nodes)
        # Everything else must be explicitly referenced with $
        AUTO_INJECT_KEYS = {'model_info'}
        
        # Auto-inject only specific keys from dependency results
        for dep_id in node.get('depends_on', []):
            if dep_id in self.results:
                dep_result = self.results[dep_id]
                # Only merge specific expected keys
                if isinstance(dep_result, dict):
                    for key, value in dep_result.items():
                        if key in AUTO_INJECT_KEYS and key not in inputs:
                            inputs[key] = value
        
        # Resolve $ references
        for key, value in list(inputs.items()):
            if isinstance(value, str) and value.startswith('$'):
                ref = value[1:]
                if '.' in ref:
                    node_id, output_key = ref.split('.', 1)
                    if node_id in self.results:
                        inputs[key] = self.results[node_id].get(output_key)
                else:
                    if ref in self.results:
                        inputs[key] = self.results[ref]
        
        return inputs
    
    def _execute_function_node(self, node: Dict) -> Dict:
        """Execute a function-based node, using environment manager if needed"""
        function_name = node['function']
        inputs = self._prepare_inputs(node)
        
        # Try to use pre-loaded function first
        if hasattr(self, 'discovered_nodes') and function_name in self.discovered_nodes:
            try:
                func = self.discovered_nodes[function_name]['function']
                result = func(**inputs)
                return result
            except Exception as e:
                self.logger.warning(f"Pre-loaded function failed, trying dynamic import: {e}")
        
        # Check if this node needs an isolated environment
        if self.environment_manager:
            # Create a fake node_type from function name for environment lookup
            node_type = function_name.split('.')[-1].replace('_node', '_node')
            
            # Extract node metadata from decorator (if function is already loaded)
            node_metadata = None
            func = None
            
            # Try to get function from discovered_nodes first
            if hasattr(self, 'discovered_nodes') and function_name in self.discovered_nodes:
                func = self.discovered_nodes[function_name]['function']
            else:
                # Function not discovered - try to import it to get metadata
                try:
                    module_name, func_name = function_name.rsplit('.', 1)
                    import importlib
                    module = importlib.import_module(module_name)
                    func = getattr(module, func_name)
                except Exception as e:
                    self.logger.debug(f"Could not import {function_name} for metadata: {e}")
            
            # Extract metadata if we have the function
            if func and (hasattr(func, 'dependencies') or hasattr(func, 'environment') or hasattr(func, 'isolation_mode')):
                node_metadata = {
                    'dependencies': getattr(func, 'dependencies', []),
                    'environment': getattr(func, 'environment', None),
                    'isolation_mode': getattr(func, 'isolation_mode', 'auto')
                }
            
            env_info = self.environment_manager.get_environment_for_node(
                node_type=node_type,
                node_config=node,
                workflow_config=self.workflow_config,
                node_metadata=node_metadata  # Pass decorator metadata
            )
            
            # Debug log for environment info
            self.logger.debug(f"{node['id']}: env_info={env_info}")
            
            # If environment is isolated, execute in that environment
            if env_info and env_info.get('is_isolated'):
                self.logger.info(f"Executing {node['id']} in {env_info['env_name']}")
                
                try:
                    result = self._execute_in_environment(
                        function_name, inputs, env_info, node['id']
                    )
                    return result
                except Exception as e:
                    self.logger.warning(f"Isolated execution failed: {e}")
                    # Fall through to direct execution
        
        # Direct execution (no isolation needed or available)
        module_name, func_name = function_name.rsplit('.', 1)
        
        try:
            # Dynamic import
            import importlib
            module = importlib.import_module(module_name)
            func = getattr(module, func_name)
            
            # Call function with prepared inputs
            result = func(**inputs)  # Unpack inputs as keyword arguments
            return result
            
        except Exception as e:
            self.logger.error(f"Failed to execute {function_name}: {e}")
            return {'error': str(e), 'status': 'failed'}
    
    def _execute_in_environment(self, function_name: str, inputs: Dict,
                               env_info: Dict, node_id: str) -> Dict:
        """Execute function in isolated environment using subprocess with shared memory"""
        import subprocess
        import tempfile
        import json
        import time
        from utilities.shared_memory_utils import (
            dict_to_shared_memory_with_header,
            cleanup_shared_memory,
            create_shared_memory_with_header
        )
        
        module_name, func_name = function_name.rsplit('.', 1)
        python_exe = env_info['python_executable']
        
        # Create unique shared memory names
        timestamp = int(time.time() * 1000000)  # microseconds for uniqueness
        shm_input_name = f"wf_input_{node_id}_{timestamp}"
        shm_output_name = f"wf_output_{node_id}_{timestamp}"
        
        # WORKFLOW ENGINE: Create and populate input shared memory with FLAG_READY
        # dict_to_shared_memory_with_header already handles pickling!
        shm_inputs, metadata_inputs = dict_to_shared_memory_with_header(inputs, shm_input_name)
        self.logger.debug(f"[{node_id}] Created input shared memory '{shm_input_name}' with FLAG_READY")
        
        # WORKFLOW ENGINE: Pre-create output shared memory (parent owns it)
        # Allocate 10MB for output data
        output_data_size = 10 * 1024 * 1024  # 10MB
        shm_outputs, output_buf = create_shared_memory_with_header(shm_output_name, output_data_size)
        self.logger.debug(f"[{node_id}] Pre-created output shared memory '{shm_output_name}' (10MB)")
        
        # Create execution script that uses shared memory
        script_content = f'''
import sys
import os
import json

# Add current directory to path  
sys.path.insert(0, r"{os.getcwd()}")

try:
    from utilities.shared_memory_utils import (
        dict_from_shared_memory_with_header,
        attach_shared_memory_with_header,
        set_flag,
        FLAG_READY
    )
    import pickle
    from {module_name} import {func_name}
    
    # Read metadata (contains shared memory names and sizes)
    with open("metadata.json", "r") as f:
        metadata = json.load(f)
    
    # SUBPROCESS: Read inputs from shared memory (waits for FLAG_READY automatically)
    shm_in, inputs = dict_from_shared_memory_with_header(
        metadata["input"],
        wait_for_ready=True,
        timeout=30.0
    )
    
    # Execute function with unpacked inputs
    result = {func_name}(**inputs)
    
    # SUBPROCESS: Attach to output shared memory (parent pre-created it)
    shm_out, output_buf = attach_shared_memory_with_header(metadata["output"]["shm_name"])
    
    # Pickle and write result to output shared memory
    result_bytes = pickle.dumps(result)
    result_size = len(result_bytes)
    
    if result_size > len(output_buf):
        raise ValueError(f"Result too large: {{result_size}} bytes > {{len(output_buf)}} bytes")
    
    output_buf[:result_size] = result_bytes
    
    # SUBPROCESS: Set FLAG_READY to signal parent
    set_flag(shm_out.buf, FLAG_READY)
    
    # Write result size for parent
    with open("result_size.txt", "w") as f:
        f.write(str(result_size))
    
    # SUBPROCESS: Clean up - release buffer references first
    del output_buf  # Release memoryview reference
    del result_bytes  # Release bytes reference
    
    # Write success marker first (before cleanup that might fail)
    with open("success.txt", "w") as f:
        f.write("OK")
    
    # Now close shared memories (parent still owns them)
    # Wrap in try/except to handle buffer reference issues gracefully
    try:
        shm_in.close()
        shm_out.close()
    except BufferError:
        # Buffer still has references - cleanup will happen via garbage collection
        pass
        
except Exception as e:
    import traceback
    with open("error.txt", "w") as f:
        f.write(str(e) + "\\n")
        f.write(traceback.format_exc())
    sys.exit(1)
'''
        
        try:
            # Create temporary directory for communication
            with tempfile.TemporaryDirectory() as temp_dir:
                script_path = os.path.join(temp_dir, "execute.py")
                metadata_path = os.path.join(temp_dir, "metadata.json")
                result_size_path = os.path.join(temp_dir, "result_size.txt")
                success_path = os.path.join(temp_dir, "success.txt")
                error_path = os.path.join(temp_dir, "error.txt")
                
                # Write script
                with open(script_path, 'w') as f:
                    f.write(script_content)
                
                # Write metadata (contains shared memory names and data sizes)
                metadata = {
                    "input": metadata_inputs,
                    "output": {"shm_name": shm_output_name}
                }
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f)
                
                # WORKFLOW ENGINE: Launch subprocess (inputs already FLAG_READY)
                self.logger.debug(f"[{node_id}] Launching subprocess...")
                result = subprocess.run(
                    [str(python_exe), script_path],
                    cwd=temp_dir,
                    capture_output=True, 
                    text=True, 
                    timeout=300,
                    encoding='utf-8',
                    errors='replace'
                )
                
                # Log subprocess output for debugging
                if result.stdout:
                    for line in result.stdout.strip().split('\n'):
                        if line.strip():
                            self.logger.info(f"[{node_id}] {line}")
                
                if result.returncode == 0 and os.path.exists(success_path):
                    # WORKFLOW ENGINE: Wait for subprocess to set FLAG_READY on output
                    from utilities.shared_memory_utils import wait_for_flag, FLAG_READY
                    import pickle
                    
                    self.logger.debug(f"[{node_id}] Waiting for FLAG_READY on output...")
                    if not wait_for_flag(shm_outputs.buf, FLAG_READY, timeout=30.0):
                        raise TimeoutError("Subprocess did not complete within timeout")
                    
                    # Read result size
                    with open(result_size_path, 'r') as f:
                        result_size = int(f.read().strip())
                    
                    # WORKFLOW ENGINE: Read result from output shared memory
                    # CRITICAL: Copy bytes completely before ANY cleanup
                    output_buf_data = shm_outputs.buf[8:]  # Skip header  
                    result_bytes = bytes(output_buf_data[:result_size])  # Full copy
                    del output_buf_data  # Release memoryview reference
                    
                    # Unpickle immediately (before cleanup) to ensure data integrity
                    output_data = pickle.loads(result_bytes)
                    del result_bytes  # Release bytes reference
                    
                    # Force deep copy of numpy arrays if present
                    if isinstance(output_data, dict):
                        import numpy as np
                        output_data_copy = {}
                        for key, value in output_data.items():
                            if isinstance(value, np.ndarray):
                                output_data_copy[key] = value.copy()  # Force numpy copy
                            else:
                                output_data_copy[key] = value
                        del output_data
                    else:
                        output_data_copy = output_data
                    
                    # WORKFLOW ENGINE: Cleanup shared memory (parent controls lifecycle)
                    # Use try/except to handle buffer reference issues gracefully
                    try:
                        shm_inputs.close()
                        shm_outputs.close()
                        cleanup_shared_memory(shm_input_name)
                        cleanup_shared_memory(shm_output_name)
                    except BufferError:
                        # Buffer still has references - cleanup will happen via garbage collection
                        self.logger.debug(f"[{node_id}] Delayed shared memory cleanup (buffer references exist)")
                        pass
                    
                    self.logger.debug(f"[{node_id}] Successfully read result from shared memory")
                    
                    return output_data_copy
                else:
                    # Read error
                    error_msg = "Unknown error"
                    if os.path.exists(error_path):
                        with open(error_path, 'r') as f:
                            error_msg = f.read()
                    elif result.stderr:
                        error_msg = result.stderr
                    
                    # WORKFLOW ENGINE: Cleanup on error
                    shm_inputs.close()
                    cleanup_shared_memory(shm_input_name)
                    cleanup_shared_memory(shm_output_name)
                    
                    raise RuntimeError(f"Process failed: {error_msg}")
                    
        except Exception as e:
            self.logger.error(f"Environment execution failed: {e}")
            # WORKFLOW ENGINE: Ensure cleanup on exception
            try:
                cleanup_shared_memory(shm_input_name)
                cleanup_shared_memory(shm_output_name)
            except Exception:
                pass
            raise
                
        return {'error': 'Execution failed', 'status': 'failed'}
    
    def execute(self) -> Dict[str, Any]:
        """Execute workflow with parallel processing"""
        if not self.nodes:
            return {}
        
        workflow_name = self.workflow_config.get('name', 'Function Workflow')
        self.logger.info(f"Starting {workflow_name} ({len(self.nodes)} nodes)")
        
        start_time = time.time()
        
        # Build dependency graph
        dependency_graph = {
            node['id']: node.get('depends_on', []) 
            for node in self.nodes
        }
        nodes_by_id = {node['id']: node for node in self.nodes}
        completed = set()
        
        # Execute in waves
        max_workers = self.workflow_config.get('settings', {}).get('max_parallel_nodes', 4)
        
        while len(completed) < len(self.nodes):
            ready_nodes = self._get_ready_nodes(dependency_graph, completed)
            
            if not ready_nodes:
                remaining = set(nodes_by_id.keys()) - completed
                self.logger.error(f"Workflow blocked. Remaining: {remaining}")
                break
            
            self.logger.info(f"Executing {len(ready_nodes)} nodes...")
            
            # Execute ready nodes in parallel
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(self._execute_function_node, nodes_by_id[node_id]): node_id
                    for node_id in ready_nodes
                }
                
                for future in concurrent.futures.as_completed(futures):
                    node_id = futures[future]
                    try:
                        result = future.result()
                        self.results[node_id] = result
                        completed.add(node_id)
                        
                        status = "COMPLETED" if not result.get('error') else "FAILED"
                        self.logger.info(f"{status}: {node_id}")
                        
                    except Exception as e:
                        self.logger.error(f"FAILED: {node_id}: {e}")
                        completed.add(node_id)
                        self.results[node_id] = {'error': str(e), 'status': 'failed'}
        
        total_time = time.time() - start_time
        self.logger.info(f"Workflow completed in {total_time:.2f}s")
        
        return self.results


def run_function_workflow(workflow_file: str) -> Dict:
    """Load and execute a function-based workflow"""
    with open(workflow_file, 'r') as f:
        workflow_data = json.load(f)
    
    engine = FunctionWorkflowEngine(workflow_data)
    return engine.execute()


if __name__ == "__main__":
    import sys
    import logging
    
    if len(sys.argv) < 2:
        print("Usage: python function_workflow_engine.py <workflow.json>")
        sys.exit(1)
    
    results = run_function_workflow(sys.argv[1])
    
    # Log workflow completion summary
    logger = logging.getLogger('workflow.engine')
    # Consider nodes successful if they don't have status='failed' or an error
    successful_nodes = sum(1 for result in results.values() 
                          if result.get('status') != 'failed' and 'error' not in result)
    failed_nodes = len(results) - successful_nodes
    
    # Debug: Show which nodes are considered failed
    if failed_nodes > 0:
        for node_id, result in results.items():
            if result.get('status') == 'failed' or 'error' in result:
                logger.info(f"Node '{node_id}' marked as failed - status: {result.get('status')}, error: {result.get('error')}")
    
    if failed_nodes == 0:
        logger.info(f"Workflow completed successfully: {successful_nodes}/{len(results)} nodes executed")
    else:
        logger.warning(f"Workflow completed with issues: {successful_nodes}/{len(results)} nodes successful, {failed_nodes} failed")
    
    # Log individual node timings in a compact format
    node_timings = []
    for node_id, result in results.items():
        if 'execution_time' in result:
            timing_ms = result['execution_time'] * 1000
            node_timings.append(f"{node_id}({timing_ms:.0f}ms)")
    
    if node_timings:
        logger.info(f"ðŸ“ˆ Node timings: {', '.join(node_timings)}")