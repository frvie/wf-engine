{
  "workflow": {
    "name": "Multi-Backend YOLO Inference (DirectML + NPU + CPU)",
    "description": "Compares DirectML RTX 5090, Intel NPU, and CPU inference performance",
    "settings": {
      "max_parallel_nodes": 4
    }
  },
  
  "nodes": [
    {
      "id": "load_image",
      "function": "workflow_nodes.load_image_node.load_image_node",
      "depends_on": [],
      "inputs": {
        "image_path": "input/soccer.jpg",
        "session_namespace": "image"
      }
    },
    
    {
      "id": "directml_model",
      "function": "workflow_nodes.load_directml_model_node.load_directml_model_node",
      "depends_on": [],
      "inputs": {
        "model_path": "models/yolov8s.onnx",
        "device_id": 0
      }
    },
    
    {
      "id": "cpu_model", 
      "function": "workflow_nodes.load_cpu_model_node.load_cpu_model_node",
      "depends_on": [],
      "inputs": {
        "model_path": "models/yolov8s.onnx"
      }
    },
    
    {
      "id": "npu_model",
      "function": "workflow_nodes.load_openvino_model_node.load_openvino_model_node",
      "depends_on": [],
      "inputs": {
        "model_path": "models/yolov8s.onnx",
        "device": "NPU"
      }
    },
    
    {
      "id": "directml_inference",
      "function": "workflow_nodes.directml_inference_node.directml_inference_node",
      "depends_on": ["load_image", "directml_model"],
      "inputs": {
        "confidence_threshold": 0.25,
        "iterations": 50
      }
    },
    
    {
      "id": "cpu_inference",
      "function": "workflow_nodes.cpu_inference_node.cpu_inference_node", 
      "depends_on": ["load_image", "cpu_model"],
      "inputs": {
        "confidence_threshold": 0.25,
        "iterations": 50
      }
    },
    
    {
      "id": "npu_inference",
      "function": "workflow_nodes.npu_inference_node.npu_inference_node",
      "depends_on": ["load_image", "npu_model"],
      "inputs": {
        "confidence_threshold": 0.25,
        "iterations": 50
      }
    },
    
    {
      "id": "compare_performance",
      "function": "workflow_nodes.performance_stats_node.performance_stats_node",
      "depends_on": ["directml_inference", "cpu_inference", "npu_inference"],
      "inputs": {
        "directml_result": "$directml_inference",
        "cpu_result": "$cpu_inference",
        "npu_result": "$npu_inference"
      }
    }
  ]
}