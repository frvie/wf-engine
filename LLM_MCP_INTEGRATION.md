# LLM + AutoGen + MCP Integration

## Overview

The workflow engine now has **full LLM capabilities** using:
- **Ollama** - Local LLM inference (qwen2.5-coder:7b)
- **AutoGen** - Multi-agent collaboration framework  
- **MCP (Model Context Protocol)** - Tool exposure for LLM clients

## What Was Built

### 1. **workflow_agent_llm.py** - LLM-Powered Workflow Composer

Multi-agent system using AutoGen:

**Agents:**
- `WorkflowPlanner` - Designs workflow structure from requirements
- `PerformanceOptimizer` - Suggests optimal parameters
- `WorkflowValidator` - Checks correctness and completeness

**Features:**
- Natural language understanding with LLM reasoning
- Context-aware composition (considers hardware, performance history)
- Fallback to rule-based if Ollama unavailable
- Async/sync interfaces

**Usage:**
```python
from workflow_agent_llm import LLMWorkflowComposer

composer = LLMWorkflowComposer()
workflow = composer.compose_workflow(
    "Create a flexible pipeline for dashcam footage with good performance"
)
```

### 2. **Enhanced MCP Server** - Tool Exposure

**New Tools Added:**
1. `create_workflow_from_nl` - Generate workflows from natural language
2. `optimize_workflow` - Get optimization suggestions
3. `analyze_performance` - Analyze execution history
4. `execute_workflow_with_learning` - Execute with agentic learning

**Existing Tools:**
- `execute_workflow` - Run workflows
- `detect_devices` - Hardware detection
- `list_workflow_nodes` - Available nodes
- `validate_workflow` - Workflow validation  
- `get_workflow_templates` - Template library

**Total: 9 MCP Tools**

### 3. **Integration Layer** - Seamless Workflow

The system integrates three approaches:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rule-Based      â”‚ LLM-Enhanced     â”‚ MCP-Exposed        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Fast (instant)  â”‚ Smart (reasons)  â”‚ Accessible (tools) â”‚
â”‚ No dependencies â”‚ Needs Ollama     â”‚ For LLM clients    â”‚
â”‚ Deterministic   â”‚ Contextual       â”‚ Claude, Continue   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## How to Use

### Option 1: Direct LLM Composition

```python
from workflow_agent_llm import LLMWorkflowComposer, verify_ollama_connection

# Check Ollama
if verify_ollama_connection():
    composer = LLMWorkflowComposer()
    
    workflow = composer.compose_workflow(
        "Fast video detection for real-time processing"
    )
    
    # Workflow is automatically optimized with:
    # - Multi-agent reasoning
    # - Historical performance data
    # - Hardware-aware backend selection
```

### Option 2: MCP Server (Claude Desktop)

1. **Configure Claude Desktop** (`claude_desktop_config.json`):
```json
{
  "mcpServers": {
    "workflow-engine": {
      "command": "uv",
      "args": ["run", "python", "mcp_server.py"],
      "cwd": "C:/dev/workflow_engine"
    }
  }
}
```

2. **Restart Claude Desktop**

3. **Use Natural Language**:
- "Create a fast video detection workflow"
- "Optimize my granular workflow for 20 FPS"
- "Analyze performance trends"
- "What's the best strategy for real-time processing?"

### Option 3: Rule-Based Fallback (No LLM)

```python
from agentic_integration import create_workflow_from_natural_language

# Works without Ollama - uses keyword matching
workflow = create_workflow_from_natural_language(
    "Detect objects in video with good performance"
)
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM Layer (New)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  workflow_agent_llm.py                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Planner    â”‚â†’ â”‚  Optimizer   â”‚â†’ â”‚  Validator   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â†“ AutoGen Multi-Agent â†“                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama (qwen2.5-coder:7b)                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MCP Layer (Enhanced)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  mcp_server.py - 9 Tools                                â”‚
â”‚  â€¢ create_workflow_from_nl ğŸ†•                           â”‚
â”‚  â€¢ optimize_workflow ğŸ†•                                 â”‚
â”‚  â€¢ analyze_performance ğŸ†•                               â”‚
â”‚  â€¢ execute_workflow_with_learning ğŸ†•                    â”‚
â”‚  â€¢ execute_workflow, detect_devices, etc.               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Agentic Layer (Preserved)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  workflow_agent.py                                      â”‚
â”‚  â€¢ WorkflowComposer (rule-based fallback)              â”‚
â”‚  â€¢ PerformanceOptimizer (history-based)                â”‚
â”‚  â€¢ PipelineSelector (strategy selection)               â”‚
â”‚  â€¢ ExecutionLearner (knowledge base)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Core Workflow Engine (Intact)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Lazy loading                                         â”‚
â”‚  â€¢ Wave parallelism                                     â”‚
â”‚  â€¢ Shared memory with headers                           â”‚
â”‚  â€¢ Self-isolation for conflicts                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Comparison

| Method | Speed | Accuracy | Use Case |
|--------|-------|----------|----------|
| **Rule-Based** | Instant | Good | Simple requirements, offline |
| **LLM (AutoGen)** | ~2-5s | Excellent | Complex requirements, contextual |
| **Hybrid** | Auto-selects | Best of both | Production (tries LLM, falls back) |

## Demo Output

```bash
$ uv run python demo_llm_mcp.py

ğŸ¤– LLM + AutoGen + MCP Integration Demo
========================================

âœ… Ollama is ready!
   Available models: qwen2.5-coder:7b, llama3.3:70B, ...

ğŸ“ Rule-Based: "Fast video detection"
   â†’ Strategy: fast_pipeline, Nodes: 5, FPS: ~25

ğŸ§  LLM-Powered: "Flexible dashcam pipeline with customization"
   â†’ Multi-agent reasoning active
   â†’ Planner designed structure
   â†’ Optimizer suggested params
   â†’ Validator checked correctness
   â†’ Strategy: granular, Nodes: 4

ğŸ”§ MCP Server: 9 tools exposed
   Ready for Claude Desktop integration
```

## Key Benefits

### LLM Enhancement Provides:

1. **Natural Language Understanding**
   - Complex requirements: "Use Intel NPU if available, otherwise GPU, with balanced performance"
   - Contextual interpretation
   - Handles ambiguity

2. **Multi-Agent Reasoning**
   - Planner, Optimizer, Validator collaborate
   - Validates design before execution
   - Explains rationale

3. **Context-Aware Optimization**
   - Considers hardware availability
   - Uses execution history
   - Adapts to performance trends

4. **Conversational Refinement**
   - Iterative workflow design
   - Ask follow-up questions
   - Explain trade-offs

### While Preserving:

- âœ… All core workflow engine features
- âœ… Rule-based fallback (no LLM needed)
- âœ… Performance (agentic learning)
- âœ… Flexibility (all 3 strategies)

## Files Created

1. **workflow_agent_llm.py** (520 lines)
   - LLMWorkflowComposer class
   - LLMWorkflowAnalyzer class
   - AutoGen agent configurations
   - Ollama integration

2. **mcp_server.py** (enhanced, 590 lines)
   - 4 new MCP tools
   - LLM/rule-based routing
   - Agentic optimization integration

3. **demo_llm_mcp.py** (340 lines)
   - Comprehensive demonstration
   - All 3 approaches tested
   - Usage examples

4. **LLM_MCP_INTEGRATION.md** (this file)
   - Complete documentation
   - Architecture diagrams
   - Usage guides

## Requirements

### For Full LLM Features:
```bash
# Install packages (already done)
uv pip install autogen-agentchat autogen-ext mcp

# Start Ollama
ollama serve

# Pull model
ollama pull qwen2.5-coder:7b
```

### For Rule-Based Only:
No additional requirements - works out of the box!

## Testing

```bash
# Test everything
uv run python demo_llm_mcp.py

# Test LLM composition only
uv run python workflow_agent_llm.py

# Test MCP server
uv run python mcp_server.py

# Test with actual execution
python agentic_integration.py workflows/granular_video_detection_mp4.json
```

## Next Steps

1. **Use with Claude Desktop**
   - Configure MCP server
   - Natural language workflow design
   - Conversational optimization

2. **Experiment with Complex Requirements**
   - "Use Intel NPU for inference, fall back to DirectML if unavailable"
   - "Maximize accuracy while staying above 15 FPS"
   - "Create a customizable pipeline for experimenting with parameters"

3. **Build Knowledge Base**
   - Run more workflows
   - Let system learn patterns
   - Better optimization suggestions

4. **Try Different Models**
   - qwen2.5-coder:7b (default, balanced)
   - llama3.3:70B (more powerful)
   - phi4:latest (faster, lighter)

## Summary

âœ… **Fully operational LLM-powered workflow engine**
âœ… **3 usage modes: Rule-based, LLM, MCP**
âœ… **All core features preserved**
âœ… **Ready for Claude Desktop integration**
âœ… **Tested and documented**

**Result:** You can now design workflows by simply describing what you want in natural language, and the LLM agents will reason about the best approach, optimize parameters, and validate the design - all while preserving the performance and flexibility of the core engine! ğŸš€
