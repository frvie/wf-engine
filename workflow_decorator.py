#!/usr/bin/env python3
"""
Unified Workflow Node Decorator Template

Single, comprehensive decorator for all workflow nodes with:
- Performance tracking
- Error handling  
- Input validation
- Global caching
- Hybrid execution (in-process/isolated)
- Environment management
- Process isolation
- Automatic conflict detection

Usage:
    @workflow_node(node_id="my_node", dependencies=["numpy"])
    def my_function(input1: str, input2: int = 10) -> Dict[str, Any]:
        return {"result": input1 * input2}
"""

import functools
import time
import sys
import os
import inspect
import pickle
import subprocess
import multiprocessing
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Callable
import traceback

# Add utilities to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utilities.logging_config import get_inference_logger
from utilities.shared_memory_utils import (
    dict_to_shared_memory_with_header,
    dict_from_shared_memory_with_header,
    pickle_to_shared_memory,
    pickle_from_shared_memory
)

# Global cache for models and expensive resources
_GLOBAL_CACHE = {}

# Environment manager instance
_ENV_MANAGER = None


def _get_environment_manager():
    """Get or create the environment manager instance"""
    global _ENV_MANAGER
    if _ENV_MANAGER is None:
        try:
            from workflow_environment_manager import WorkflowEnvironmentManager
            _ENV_MANAGER = WorkflowEnvironmentManager()
        except ImportError:
            _ENV_MANAGER = None  # No environment manager available
    return _ENV_MANAGER


def _determine_execution_mode(
    isolation_mode: str,
    environment: Optional[str],
    dependencies: List[str],
    logger
) -> str:
    """
    Determine whether to run in-process or isolated
    
    Returns: "in_process", "isolated", or "environment"
    """
    if isolation_mode == "never":
        return "in_process"
    elif isolation_mode == "always" or isolation_mode == "isolated":
        return "isolated" if environment is None else "environment"
    elif isolation_mode == "auto":
        # Auto-detect conflicts (simplified conflict detection)
        conflict_libraries = [
            "onnxruntime-directml", "onnxruntime-gpu",
            "torch", "tensorflow", "openvino"
        ]
        
        has_conflicts = any(dep in conflict_libraries for dep in dependencies)
        if has_conflicts:
            logger.debug(f"ðŸ”§ Detected potential conflicts in {dependencies}")
            return "isolated" if environment is None else "environment"
        else:
            return "in_process"
    else:
        return "in_process"


def _execute_in_process(func: Callable, inputs: Dict[str, Any], logger) -> Dict[str, Any]:
    """Execute function in current process (fastest)"""
    logger.debug("ðŸš€ Executing in-process")
    return func(**inputs)


def _execute_isolated(func: Callable, inputs: Dict[str, Any], node_id: str, logger) -> Dict[str, Any]:
    """Execute function in isolated process with shared memory."""
    try:
        logger.info(f"ðŸ”’ Executing {func.__name__} in isolated process")
        
        # Create shared memory for inputs using pickle approach
        input_metadata = pickle_to_shared_memory(inputs, f"input_{node_id}")
        
        # Create the subprocess execution script
        script_content = f"""
import sys
import os
import pickle
sys.path.append(r'{os.path.dirname(os.path.dirname(os.path.abspath(__file__)))}')
from utilities.shared_memory_utils import pickle_from_shared_memory, pickle_to_shared_memory

def execute_function():
    try:
        # Read inputs from shared memory
        input_metadata = {input_metadata}
        inputs = pickle_from_shared_memory(input_metadata)
        
        # Define and execute the function (extract only the function body)
        func_source = '''{inspect.getsource(func).split('def')[1]}'''
        func_code = 'def ' + func_source
        
        # Execute the function definition
        local_vars = {{}}
        exec(func_code, globals(), local_vars)
        
        # Get the function from local variables
        func_obj = local_vars['{func.__name__}']
        
        # Execute function
        result = func_obj(**inputs)
        
        # Write results to shared memory
        output_metadata = pickle_to_shared_memory(result, "output_{node_id}")
        
        # Print metadata for parent process to read
        print("OUTPUT_METADATA:", output_metadata)
        
        return True
    except Exception as e:
        print(f"Error in subprocess: {{e}}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = execute_function()
    sys.exit(0 if success else 1)
"""
        
        # Write script to temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py',
                                       delete=False) as f:
            f.write(script_content)
            script_path = f.name
        
        try:
            # Execute subprocess
            result = subprocess.run([sys.executable, script_path],
                                  capture_output=True, text=True,
                                  timeout=300)
            
            if result.returncode == 0:
                # Parse output metadata from subprocess output
                output_lines = result.stdout.strip().split('\n')
                output_metadata = None
                for line in output_lines:
                    if line.startswith('OUTPUT_METADATA:'):
                        output_metadata = eval(line.split(':', 1)[1].strip())
                        break
                
                if output_metadata:
                    outputs = pickle_from_shared_memory(output_metadata)
                    logger.info("âœ… Isolated execution completed successfully")
                    return outputs
                else:
                    raise RuntimeError("No output metadata found")
            else:
                logger.error(f"âŒ Subprocess failed: {result.stderr}")
                raise RuntimeError(f"Subprocess failed: {result.stderr}")
                
        finally:
            # Cleanup
            os.unlink(script_path)
            
    except Exception as e:
        logger.error(f"âŒ Isolated execution failed: {e}")
        logger.warning("âš ï¸ Falling back to in-process execution")
        return _execute_in_process(func, inputs, logger)


def _execute_in_environment(
    func: Callable,
    inputs: Dict[str, Any],
    environment: str,
    node_id: str,
    logger
) -> Dict[str, Any]:
    """Execute function in specific environment (delegated to workflow engine)."""
    logger.info(f"ðŸŒ Executing {func.__name__} in environment: {environment}")
    
    # For now, fall back to in-process execution
    # The workflow engine will handle environment execution at a higher level
    logger.debug("Environment execution delegated to workflow engine")
    return _execute_in_process(func, inputs, logger)


def workflow_node(
    node_id: str,
    dependencies: Optional[List[str]] = None,
    cache_models: bool = True,
    isolation_mode: str = "auto",  # "auto", "always", "never", "in_process", "none"
    environment: Optional[str] = None,
    performance_tracking: bool = True,
    log_level: str = "INFO"  # Control logging verbosity
):
    """
    Universal workflow node decorator with hybrid execution capabilities
    
    Environment auto-generation:
    - If dependencies are specified and isolation is needed, environment is auto-created
    - No need for external environments.json file
    - Environment name is auto-generated from dependencies
    
    Args:
        node_id: Unique identifier for this node
        dependencies: List of required Python packages (auto-creates environment if needed)
        cache_models: Whether to enable global model caching
        isolation_mode: Execution isolation mode:
            - "auto": Auto-detect conflicts, isolate if needed
            - "always"/"isolated": Always run in isolated environment
            - "never"/"in_process"/"none": Always run in main process
        environment: Explicit environment name (optional, auto-generated if not specified)
        performance_tracking: Enable timing and performance metrics
        log_level: Logging verbosity level
        
    Example:
        @workflow_node(node_id="directml_inference", 
                      dependencies=["onnxruntime-directml", "numpy"])
        def my_node(image: np.ndarray) -> Dict[str, Any]:
            # Environment auto-created with specified dependencies
            import onnxruntime as ort
            return {"result": "success"}
    """
    def decorator(func: Callable) -> Callable:
        # Get function signature for parameter filtering
        sig = inspect.signature(func)
        func_params = set(sig.parameters.keys())
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Dict[str, Any]:
            logger = get_inference_logger(node_id)
            start_time = time.perf_counter() if performance_tracking else None
            
            # Log node execution start
            if log_level in ["DEBUG", "INFO"]:
                logger.info(f"Starting {node_id}")
                if log_level == "DEBUG":
                    logger.debug(f"Input args: {len(args)} positional, "
                               f"{len(kwargs)} keyword arguments")
            
            try:
                # Validate dependencies if specified
                if dependencies:
                    # Don't block execution for missing dependencies
                    # Let environment manager handle this or let function fail naturally
                    missing_deps = _check_dependencies(dependencies, logger)
                    if missing_deps:
                        logger.debug(f"âš ï¸ Dependencies not in main env: {missing_deps}")
                        logger.debug("Environment manager may provide these via isolation")
                    elif log_level == "DEBUG":
                        logger.debug(f"âœ… All dependencies satisfied")
                
                # Determine execution mode
                exec_mode = _determine_execution_mode(
                    isolation_mode, environment, dependencies or [], logger
                )
                
                if log_level == "DEBUG":
                    logger.debug(f"ðŸ”§ Execution mode: {exec_mode}")
                
                # Filter inputs to only include function parameters
                if len(args) == 1 and isinstance(args[0], dict) and not kwargs:
                    # Called with single dict input (common workflow pattern)
                    all_inputs = args[0]
                    filtered_inputs = {k: v for k, v in all_inputs.items()
                                     if k in func_params}
                    
                    if log_level == "DEBUG":
                        filtered_out = set(all_inputs.keys()) - func_params
                        if filtered_out:
                            logger.debug(f"ðŸ”§ Filtered out: {filtered_out}")
                        logger.debug(f"ðŸ”§ Using: {list(filtered_inputs.keys())}")
                    
                    # Execute based on determined mode
                    if exec_mode == "in_process":
                        result = _execute_in_process(func, filtered_inputs, logger)
                    elif exec_mode == "isolated":
                        result = _execute_isolated(func, filtered_inputs, node_id, logger)
                    elif exec_mode == "environment":
                        result = _execute_in_environment(
                            func, filtered_inputs, environment, node_id, logger
                        )
                    else:
                        result = _execute_in_process(func, filtered_inputs, logger)
                        
                else:
                    # Called with individual arguments - only support in-process
                    if log_level == "DEBUG":
                        logger.debug(f"ðŸ”§ Direct function call with {len(args)} args")
                    result = func(*args, **kwargs)
                
                # Add metadata and logging if result is a dict
                if isinstance(result, dict):
                    if performance_tracking and start_time:
                        execution_time = time.perf_counter() - start_time
                        result['_node_metadata'] = {
                            'node_id': node_id,
                            'execution_time': execution_time,
                            'execution_mode': exec_mode,
                            'status': 'success',
                            'cache_enabled': cache_models
                        }
                        
                        # Log completion with performance info
                        if execution_time > 1.0:  # Log slow operations prominently
                            logger.warning(f"â±ï¸  {node_id} completed in {execution_time:.3f}s (slow)")
                        else:
                            logger.info(f"âœ… {node_id} completed in {execution_time:.3f}s")
                        
                        if log_level == "DEBUG":
                            logger.debug(f"ðŸ“Š Performance: {execution_time*1000:.1f}ms")
                    else:
                        logger.info(f"{node_id} completed successfully")
                
                return result
                
            except Exception as e:
                error_msg = f"Node {node_id} failed: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                if log_level == "DEBUG":
                    logger.error(traceback.format_exc())
                
                return _error_result(error_msg, node_id, start_time)
        
        # Add node metadata to the wrapper
        wrapper.node_id = node_id
        wrapper.dependencies = dependencies or []
        wrapper.cache_models = cache_models
        wrapper.isolation_mode = isolation_mode
        wrapper.environment = environment
        wrapper.performance_tracking = performance_tracking
        wrapper.log_level = log_level
        
        # Add utility methods
        wrapper.get_cache = lambda key: _GLOBAL_CACHE.get(f"{node_id}:{key}")
        wrapper.set_cache = lambda key, value: _GLOBAL_CACHE.update(
            {f"{node_id}:{key}": value}
        )
        wrapper.clear_cache = lambda: _clear_node_cache(node_id)
        wrapper.validate_deps = lambda: _check_dependencies(
            dependencies or [], get_inference_logger(node_id)
        )
        
        # Add logger access for custom logging in node functions
        wrapper.get_logger = lambda: get_inference_logger(node_id)
        wrapper.log_info = lambda msg: get_inference_logger(node_id).info(msg)
        wrapper.log_warning = lambda msg: get_inference_logger(node_id).warning(msg)
        wrapper.log_error = lambda msg: get_inference_logger(node_id).error(msg)
        wrapper.log_debug = lambda msg: get_inference_logger(node_id).debug(msg)
        
        return wrapper
    
    return decorator


def _check_dependencies(dependencies: List[str], logger) -> List[str]:
    """Check if dependencies are available, return list of missing ones"""
    import importlib.util
    
    missing = []
    for dep in dependencies:
        try:
            spec = importlib.util.find_spec(dep.replace('-', '_'))
            if spec is None:
                missing.append(dep)
        except ImportError:
            missing.append(dep)
        except Exception as e:
            logger.warning(f"Could not check dependency {dep}: {e}")
            missing.append(dep)
    
    return missing


def _error_result(error_msg: str, node_id: str, start_time: Optional[float]) -> Dict[str, Any]:
    """Create standardized error result"""
    result = {
        'error': error_msg,
        'status': 'failed',
        '_node_metadata': {
            'node_id': node_id,
            'status': 'failed'
        }
    }
    
    if start_time:
        execution_time = time.perf_counter() - start_time
        result['_node_metadata']['execution_time'] = execution_time
    
    return result


def _clear_node_cache(node_id: str):
    """Clear cache entries for a specific node"""
    keys_to_remove = [k for k in _GLOBAL_CACHE.keys() if k.startswith(f"{node_id}:")]
    for key in keys_to_remove:
        del _GLOBAL_CACHE[key]


def clear_all_cache():
    """Clear the entire global cache"""
    global _GLOBAL_CACHE
    _GLOBAL_CACHE.clear()


def get_cache_info() -> Dict[str, Any]:
    """Get information about the global cache"""
    return {
        'total_entries': len(_GLOBAL_CACHE),
        'cache_keys': list(_GLOBAL_CACHE.keys()),
        'memory_usage_estimate': sum(
            sys.getsizeof(v) for v in _GLOBAL_CACHE.values()
        )
    }


# Convenience aliases for specific node types
def model_loader_node(
    node_id: str,
    dependencies: Optional[List[str]] = None,
    log_level: str = "INFO",
    **kwargs
):
    """Convenience decorator for model loader nodes"""
    default_deps = dependencies or ["numpy"]
    return workflow_node(
        node_id=node_id,
        dependencies=default_deps,
        cache_models=True,  # Always cache models
        log_level=log_level,
        **kwargs
    )


def inference_node(
    node_id: str,
    dependencies: Optional[List[str]] = None,
    log_level: str = "INFO",
    **kwargs
):
    """Convenience decorator for inference nodes"""
    default_deps = dependencies or ["numpy"]
    return workflow_node(
        node_id=node_id,
        dependencies=default_deps,
        cache_models=False,  # Inference doesn't typically cache
        log_level=log_level,
        **kwargs
    )


def utility_node(
    node_id: str,
    dependencies: Optional[List[str]] = None,
    log_level: str = "INFO",
    **kwargs
):
    """Convenience decorator for utility nodes (image loading, stats, etc.)"""
    return workflow_node(
        node_id=node_id,
        dependencies=dependencies,
        cache_models=False,
        log_level=log_level,
        **kwargs
    )


if __name__ == "__main__":
    # Example usage and testing
    print("ðŸŽ¯ Unified Workflow Node Decorator with Hybrid Execution")
    print("=" * 60)
    
    # Test the decorator
    @workflow_node(node_id="test_node", dependencies=["numpy"], isolation_mode="auto")
    def test_function(x: int, y: int = 10) -> Dict[str, Any]:
        import numpy as np
        result = np.array([x, y]).sum()
        return {"result": int(result), "inputs": {"x": x, "y": y}}
    
    # Test with dict input (workflow pattern)
    dict_result = test_function({"x": 5, "y": 15, "extra_param": "ignored"})
    print(f"âœ… Dict input result: {dict_result}")
    
    # Test with individual arguments
    args_result = test_function(3, 7)
    print(f"âœ… Args input result: {args_result}")
    
    # Test cache
    test_function.set_cache("test_key", "test_value")
    cached_value = test_function.get_cache("test_key")
    print(f"âœ… Cache test: {cached_value}")
    
    # Test dependency validation
    missing = test_function.validate_deps()
    print(f"âœ… Dependency check: missing={missing}")
    
    # Show cache info
    cache_info = get_cache_info()
    print(f"âœ… Cache info: {cache_info}")
    
    # Test conflict detection
    @workflow_node(node_id="conflict_test", dependencies=["onnxruntime-directml"], isolation_mode="auto")
    def conflict_function(data: str) -> Dict[str, Any]:
        return {"result": f"processed_{data}"}
    
    conflict_result = conflict_function({"data": "test"})
    print(f"âœ… Conflict detection test: {conflict_result}")
    
    print("\nðŸš€ Hybrid decorator ready for use!")
    print("\nKey Features:")
    print("  âœ… Automatic conflict detection")
    print("  âœ… Hybrid execution (in-process/isolated/environment)")
    print("  âœ… Global model caching")
    print("  âœ… Comprehensive logging")
    print("  âœ… Environment manager integration")
    print("  âœ… Performance tracking")